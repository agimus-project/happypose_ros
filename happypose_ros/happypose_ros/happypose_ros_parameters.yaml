happypose_ros:
  device:
    type: string
    default_value: "cpu"
    description: "Device to which the models will be loaded. Supported options are 'cpu' and 'cuda:x' where 'x' is the GPU number."
    validation:
      custom_validators::torch_check_device:
    read_only: true
  verbose_info_logs:
    type: bool
    default_value: false
    description: "Extended verbosity on info logs. Show logs with number of detections and more."
  time_stamp_strategy:
    type: string
    default_value: "oldest"
    description: "Which image time stamp to use in final detection message."
    validation:
      one_of<>: [["average", "newest", "oldest"]]
  visualization:
    publish_markers:
      type: bool
      default_value: false
      description: "Publish detected objects as markers to visualize detections in RViz."
      read_only: true
    markers:
      dynamic_opacity:
        type: bool
        default_value: false
        description: "Change opacity of published markers based on their prediction score."
      lifetime:
        type: double
        default_value: 10.0
        description: "Lifetime of a published marker."
        validation:
          gt<>: [0.0]
  use_depth:
    type: bool
    default_value: false
    description: "Specifies whether to use depth images for pose refinement. If set to `true` all cameras are expected to provide depth images."
    read_only: true
  pose_estimator_type:
    type: string
    default_value: "cosypose"
    description: "Specifies which pose estimator to use in the pipeline."
    validation:
      one_of<>: [["cosypose", "megapose"]]
    read_only: true
  cosypose:
    dataset_name:
      type: string
      default_value: ""
      description: "Name of BOP dataset, used to load specific weights and object models."
      validation:
        one_of<>: [["hope", "tless", "ycbv", ""]]
      read_only: true
    model_type:
      type: string
      default_value: "pbr"
      description: "Type of neural network model to use. Available: 'pbr'|'synth+real'"
      validation:
        one_of<>: [["pbr", "synth+real", ""]]
      read_only: true
    depth_refiner_type:
      type: string
      default_value: "icp"
      description: "Type of happypose depth refinement. Available: 'icp'"
      validation:
        one_of<>: [["icp"]]
        custom_validators::check_teaserpp_installed:
      read_only: true
    renderer:
      renderer_type:
        type: string
        default_value: "panda3d"
        description: "Specifies which renderer to use in the pipeline."
        validation:
          one_of<>: [["panda3d", "bullet"]]
        read_only: true
      n_workers:
        type: int
        default_value: 8
        description: "Number of CPU cores to use during rendering."
        validation:
          gt_eq<>: 1
        read_only: true
      use_antialiasing:
        type: bool
        default_value: true
        description: "Use antialiasing in the rendering process. Slower and does not increase much performance. Only for panda3d. Enabled by default for backwards compatibility."
        read_only: true
    inference:
      detector:
        detection_th:
          type: double
          default_value: 0.7
          description: "Detection threshold of an object used by detector."
          validation:
            bounds<>: [0.0, 1.0]
      pose_estimator:
        n_refiner_iterations:
          type: int
          default_value: 3
          description: "Number of iterations for the refiner."
          validation:
            gt_eq<>: [1]
        n_coarse_iterations:
          type: int
          default_value: 1
          description: "Number of iterations for the coarse estimate."
          validation:
            gt_eq<>: [1]
      icp:
        n_min_points:
          type: int
          default_value: 1000
          description: "Minimum number of matching depth points to consider detection valid."
          validation:
            gt_eq<>: [1]
        min_measured_depth:
          type: double
          default_value: 0.2
          description: "Minimum clipping distance of depth point from the camera to be considered valid."
          validation:
            gt_eq<>: [0.0]
        max_measured_depth:
          type: double
          default_value: 5.0
          description: "Maximum clipping distance of depth point from the camera to be considered valid."
          validation:
            gt_eq<>: [0.0]
        iterations:
          type: int
          default_value: 100
          description: "Number of iterations of ICP algorithm."
          validation:
            gt_eq<>: [1]
        tolerance:
          type: double
          default_value: 0.05
          description: "Tells the algorithm to stop when the norm of the error vector makes less improvement in percent, than tolerance value. The error function is the norm of the difference of the matched 3D points."
          validation:
            gt_eq<>: [0.0]
        rejection_scale:
          type: double
          default_value: 2.5
          description: "Robust outlier rejection is applied for robustness. This value actually corresponds to the standard deviation coefficient. Points with rejectionScale * &sigma are ignored during registration."
          validation:
            gt_eq<>: [0.0]
        num_levels:
          type: int
          default_value: 4
          description: "Corresponds to iterative ICP, where the original point cloud is subsampled. Note that if the output of CosyPose is considered good, ``num_levels`` does not have to be high."
          validation:
            gt_eq<>: [1]
      multiview:
        ransac_n_iter:
          type: int
          default_value: 2000
          description: "Number of ransac iterations when matching views."
        ransac_dist_threshold:
          type: double
          default_value: 0.2
          description: "Threshold (in metter) on the symmetric distance (Mean Symmetry-Aware Surface Distance) used consider a tentative match as an inlier during RANSAC iterations."
        ba_n_iter:
          type: int
          default_value: 100
          description: "Number of steps in the final bundle adjustment refinement."
      labels_to_keep:
        type: string_array
        description: "Labels of detected objects to keep. If not specified, all objects are kept."
        # Walkaround to obtain "empty" string array
        default_value: [""]
        validation:
          unique<>:
  megapose:
    model_name:
      type: string
      default_value: "megapose-1.0-RGB-multi-hypothesis"
      description: "Name of the megapose model to be used in inference, for now no model with depth is supported."
      validation:
        one_of<>: [["megapose-1.0-RGB-multi-hypothesis", "megapose-1.0-RGB"]]
      read_only: true
    mesh:
      directory_path:
        type: string
        default_value: "src/happypose_ros/happypose_examples/resources/meshes"
        description: "Path to the mesh directory."
      units:
        type: string
        default_value: "mm"
        description: "Unit of the mesh."
        validation:
          one_of<>: [["mm"]]
        read_only: true
      file_extension:
        type: string
        default_value: "obj"
        description: "Filetype of the mesh."
        validation:
          one_of<>: [["ply", "obj"]]
        read_only: true
    subsample_scale:
      type: int
      default_value: 8
      description: "Subsampling scale value"
      read_only: true
    detector:
      detector_path:
        type: string
        default_value: "src/happypose_ros/happypose_examples/resources/yolo-checkpoints/bar-holder-stripped-bi-v3.pt"
        description: "Path to the .pt yolo model used as a detector for megapose"
      obj_label:
        type: string
        default_value: "bar-holder-stripped-bi-v3"
        description: "Yolo label of the object to detect"
  camera_names:
    type: string_array
    description: "List of names of cameras to subscribe."
    read_only: true
    validation:
      size_gt<>: [0]
      unique<>:
  cameras:
    timeout:
      type: double
      default_value: 0.0
      description: "Timeout, after which a frame from a camera is considered too old. Value '0.0' disables timeout."
      validation:
        gt_eq<>: [0.0]
    n_min_cameras:
      type: int
      default_value: 1
      description: "Minimum number of valid camera views to start pose estimation pipeline."
      validation:
        gt_eq<>: [1]
      read_only: true
    __map_camera_names:
      compressed:
        type: bool
        default_value: false
        description: "Expect compressed image messages from given camera."
        read_only: true
      leading:
        type: bool
        default_value: false
        description: "Consider the camera to be leading. If a camera is leading, its frame_id is used as a reference. Only one camera can be leading, and it can't publish TF at the same time."
        read_only: true
      publish_tf:
        type: bool
        default_value: false
        description: "Publish TF of a given camera relative to the leading camera."
        read_only: true
      estimated_tf_frame_id:
        type: string
        default_value: ""
        description: "Name of frame_id published for estimated camera poses. If empty string, frame_id of camera is used. Leading camera can not have estimated frame_id."
        validation:
          custom_validators::check_tf_valid_name:
      time_sync_slop:
        type: double
        default_value: 0.04
        description: "Delay [seconds] with which incoming messages can be synchronized."
        validation:
          gt<>: [0.0]
